{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "from keras.datasets import cifar10\n",
    "from skimage.transform import resize\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights\n",
    "# inception = InceptionResNetV2(weights=None, include_top=True)\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "\n",
    "# inception.load_weights('/data/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/shreyajain/Downloads/image_train_dataset/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_size_ori = 2048\n",
    "img_size_ori = 256\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    img_height = img.shape[0]\n",
    "    img_width = img.shape[1]\n",
    "    if img_size_ori == img_height and img_size_ori == img_width:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img, img_shape):\n",
    "    img_height = img_shape[0] \n",
    "    img_width = img_shape[1]\n",
    "#     if img_size_ori == img_size_target:\n",
    "#         return img\n",
    "    return resize(img, (img_height, img_width), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input  (?, 256, 256, 1)\n",
      "encoder_output  (?, 128, 128, 128)\n",
      "encoder_output  (?, 64, 64, 256)\n",
      "encoder_output  (?, 32, 32, 512)\n",
      "encoder_output  (?, 32, 32, 256)\n",
      "fusion_output  (?, 32, 32, 256)\n",
      "decoder_output  (?, 32, 32, 128)\n",
      "decoder_output  (?, 64, 64, 64)\n",
      "decoder_output  (?, 128, 128, 16)\n",
      "decoder_output  (?, 256, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "# 256 l -> ab \n",
    "embed_input = Input(shape=(1000,))\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "print (\"encoder_input \", encoder_input.shape)\n",
    "\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "print (\"encoder_output \", encoder_output.shape)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "print (\"encoder_output \", encoder_output.shape)\n",
    "\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "print (\"encoder_output \", encoder_output.shape)\n",
    "\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "print (\"encoder_output \", encoder_output.shape)\n",
    "\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "print (\"fusion_output \", fusion_output.shape)\n",
    "\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "print (\"decoder_output \", decoder_output.shape)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "print (\"decoder_output \", decoder_output.shape)\n",
    "\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "print (\"decoder_output \", decoder_output.shape)\n",
    "\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "print (\"decoder_output \", decoder_output.shape)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = \"/Users/shreyajain/Downloads/image_val_dataset/\"\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed\n",
    "\n",
    "\n",
    "#parameters\n",
    "# batch_size = 20\n",
    "batch_size = 2\n",
    "lr_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "def val_image_a_b_gen(batch_size):\n",
    "    for img_batch in datagen.flow_from_directory(directory = val_path,\n",
    "                                                        target_size=(256,256),\n",
    "                                                        color_mode='rgb',\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=42):\n",
    "\n",
    "        # print (\"tuple \", len(img_batch))\n",
    "        \n",
    "        batch = img_batch[0]\n",
    "        \n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        embed = create_inception_embedding(grayscaled_rgb)\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)\n",
    "\n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    for img_batch in datagen.flow_from_directory(directory = data_path,\n",
    "                                                        target_size=(256,256),\n",
    "                                                        color_mode='rgb',\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=42):\n",
    "\n",
    "        # print (\"tuple \", len(img_batch))\n",
    "        batch = img_batch[0]\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        embed = create_inception_embedding(grayscaled_rgb)\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        print (\"X_batch_input \", X_batch.shape)\n",
    "        print (\"Y_batch_input \", Y_batch.shape)\n",
    "        print (\"embed_input \", embed.shape)\n",
    "        print (\"X_batch\", X_batch[0][0])\n",
    "        print (\"Y_batch\", Y_batch[0][0])\n",
    "\n",
    "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"/Users/shreyajain/PycharmProjects/GAN/Image_colorisation/output\",histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# model.fit_generator(generator=image_a_b_gen(batch_size), callbacks=[tensorboard], validation_data =val_image_a_b_gen(batch_size),validation_steps=batch_size, epochs=20, steps_per_epoch=2)\n",
    "model.fit_generator(generator=image_a_b_gen(batch_size), epochs=5, steps_per_epoch=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"color_tensorflow_real_mode.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_shape  (2040, 1356, 3)\n"
     ]
    }
   ],
   "source": [
    "color_me = []\n",
    "test_path = '/Users/shreyajain/Downloads/image_test_dataset2/'\n",
    "image_shape = ()\n",
    "for filename in os.listdir(test_path):\n",
    "    image_shape = img_to_array(load_img(test_path+filename)).shape\n",
    "    color_me.append(upsample(img_to_array(load_img(test_path+filename))))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "print (\"image_shape \", image_shape)\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    img = lab2rgb(cur)\n",
    "    img_resize = resize(img, (image_shape[0], image_shape[1]), mode='constant', preserve_range=True)\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", img_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
